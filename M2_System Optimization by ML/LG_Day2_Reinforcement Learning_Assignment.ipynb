{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f183ea38",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hrbae/LG_AI_training/blob/main/M2_System%20Optimization%20by%20ML/LG_Day2_Reinforcement%20Learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd8a3df0",
   "metadata": {},
   "source": [
    "#### 과제. 작업자 배정 문제 - 추가 제약사항 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dbfdda1",
   "metadata": {},
   "source": [
    "- 목적: 총 비용 최소화\n",
    "- 제약조건 1: 각 작업자를 최소 1개, 최대 3개 작업에 할당 (기존) -> 각 작업자를 최소 2개, 최대 3개 작업에 할당 (변경)\n",
    "- 제약조건 2: 각 작업은 최소 1명의 작업자에게만 할당이 되어야 함   \n",
    "- 제약조건 3: 작업자는 2개의 팀으로 분할되며 (기존) -> 작업자는 3개의 팀으로 분할되며 (변경), 각 팀은 최소 2개 이상 최대 4개까지 작업할당이 가능\n",
    "\n",
    "| Worker | Team   | Task 0 | Task 1 | Task 2 | Task 3 | Task 4 | Task 5 | Task 6 | Task 7 | Task 8 | Task 9 | Task 10 |\n",
    "| ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------  |\n",
    "| 0      | Team 1 | 90     | 80     | 75     | 100    | 100    | 140    | 120    | 35     | 210    | 130    | 80      |\n",
    "| 1      | Team 1 | 65     | 35     | 55     | 165    | 120    | 55     | 40     | 45     | 180    | 220    | 130     |\n",
    "| 2      | Team 2 | 225    | 45     | 90     | 55     | 140    | 100    | 60     | 15     | 140    | 100    | 20      |\n",
    "| 3      | Team 2 | 35     | 20     | 95     | 315    | 50     | 80     | 80     | 35     | 100    | 60     | 90      |\n",
    "| 4      | Team 3 | 10     | 150    | 90     | 120    | 40     | 200    | 90     | 95     | 300    | 50     | 100     |\n",
    "| 5      | Team 3 | 80     | 40     | 100    | 30     | 70     | 150    | 30     | 15     | 400    | 90     | 150     |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5d2e5d9",
   "metadata": {},
   "source": [
    "##### 기존 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05401f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4521a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class taskAllocation:\n",
    "    \n",
    "    def __init__(self, epsilon, discount_factor, learning_rate, constraints):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.Action_Index = [0, 1, 2, 3, 4, 5]\n",
    "        self.Action = [0, 1, 2, 3, 4, 5]\n",
    "        \n",
    "        self.team1 , self.team2 , self.team_min , self.team_max , self.w_minimum , self.w_maximum  = constraints\n",
    "        self.QTable = defaultdict(lambda : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        \n",
    "        \n",
    "    def arg_max_Moving(self,state_Moving):\n",
    "        \n",
    "        max_index_list = []\n",
    "        max_value = state_Moving[0]\n",
    "        \n",
    "        for index, value in enumerate(state_Moving):\n",
    "            \n",
    "            if value > max_value:\n",
    "                \n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "                \n",
    "            elif value == max_value:\n",
    "                \n",
    "                max_index_list.append(index)\n",
    "                \n",
    "        return random.choice(max_index_list)\n",
    "    \n",
    "    \n",
    "    def Get_Action(self, state):\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:    \n",
    "            Action = np.random.choice(self.Action_Index)\n",
    "            \n",
    "        else:\n",
    "            state_str = [str(x) for x in state]\n",
    "            state_str2 = '.'.join(state_str)\n",
    "            '''\n",
    "            state_Moving = Allocation.QTable[state_str2]\n",
    "            '''\n",
    "            state_Moving = self.QTable[state_str2]\n",
    "            Action = self.arg_max_Moving(state_Moving = state_Moving)\n",
    "            \n",
    "        return Action\n",
    "    \n",
    "    def Q_learning(self, state, next_state, reward):\n",
    "        \n",
    "        Action = next_state[-1]\n",
    "        state_str = [str(x) for x in state]\n",
    "        state_str2 = '.'.join(state_str)\n",
    "        Q1 = self.QTable[state_str2][Action]\n",
    "        \n",
    "        nextstate_str = [str(x) for x in next_state]\n",
    "        nextstate_str2 = '.'.join(nextstate_str)\n",
    "        Q2 = reward + self.discount_factor * max(self.QTable[nextstate_str2])\n",
    "        \n",
    "        self.QTable[state_str2][Action] += self.learning_rate*(Q2-Q1)\n",
    "        \n",
    "    def Get_State(self, x, costs, steps, Action, team1, team2):\n",
    "        \n",
    "        ## Constraint 1\n",
    "        if sum(x[Action,:]) < self.w_minimum:\n",
    "            penalty1_1 = -10\n",
    "        else:\n",
    "            penalty1_1 = 0\n",
    "        if sum(x[Action,:]) > self.w_maximum:\n",
    "            penalty1_2 = -10\n",
    "        else:\n",
    "            penalty1_2 = 0\n",
    "    \n",
    "        \n",
    "        ##  Constraint 2\n",
    "        if sum(x[team1,:].sum(axis=1)) < self.team_min:\n",
    "            penalty3_1 = -10\n",
    "        else:\n",
    "            penalty3_1 = 0\n",
    "        if sum(x[team1,:].sum(axis=1)) > self.team_max:\n",
    "            penalty3_2 = -10\n",
    "        else:\n",
    "            penalty3_2 = 0\n",
    "        if sum(x[team2,:].sum(axis=1)) < self.team_min:\n",
    "            penalty3_3 = -10\n",
    "        else:\n",
    "            penalty3_3 = 0\n",
    "        if sum(x[team2,:].sum(axis=1)) > self.team_max:\n",
    "            penalty3_4 = -10\n",
    "        else:\n",
    "            penalty3_4 = 0\n",
    "            \n",
    "        cost = costs[Action][steps] * -1\n",
    "        reward = penalty1_1 + penalty1_2 + penalty3_1 + penalty3_2 + penalty3_3 + penalty3_4 + cost\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc03e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "team1 = \"변경된 상황에 맞게 구현하시오\"\n",
    "team2 = \"변경된 상황에 맞게 구현하시오\"\n",
    "team_min = \"변경된 상황에 맞게 구현하시오\"\n",
    "team_max = \"변경된 상황에 맞게 구현하시오\"\n",
    "w_minimum = \"변경된 상황에 맞게 구현하시오\"\n",
    "w_maximum = \"변경된 상황에 맞게 구현하시오\"\n",
    "\n",
    "constraints = []\n",
    "constraints.append(team1)\n",
    "constraints.append(team2)\n",
    "constraints.append(team_min)\n",
    "constraints.append(team_max)\n",
    "constraints.append(w_minimum)\n",
    "constraints.append(w_maximum)\n",
    "\n",
    "costs = [\n",
    "    [90, 80, 75, 100, 100, 140, 120, 35, 210, 130, 80],\n",
    "    [65, 35, 55, 165, 120, 55, 40, 45, 180, 220, 130],\n",
    "    [225, 45, 90, 55, 140, 100, 60, 15, 140, 100, 20],\n",
    "    [35, 20, 95, 315, 50, 80, 80, 35, 100, 60, 90],\n",
    "    [10, 150, 90, 120, 40, 200, 90, 95, 300, 50, 100],\n",
    "    [80, 40, 100, 30, 70, 150, 30, 15, 400, 90, 150]\n",
    "]\n",
    "\n",
    "num_workers = 6\n",
    "num_tasks = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc137a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.2\n",
    "\n",
    "Allocation = taskAllocation(epsilon, discount_factor, learning_rate, constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6caddadc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\test\\Dropbox\\PersonalComputer\\LG 교육\\LG_Day2_Reinforcement Learning2_과제.ipynb Cell 9\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/test/Dropbox/PersonalComputer/LG%20%EA%B5%90%EC%9C%A1/LG_Day2_Reinforcement%20Learning2_%EA%B3%BC%EC%A0%9C.ipynb#Y121sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episode):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/test/Dropbox/PersonalComputer/LG%20%EA%B5%90%EC%9C%A1/LG_Day2_Reinforcement%20Learning2_%EA%B3%BC%EC%A0%9C.ipynb#Y121sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     state \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/test/Dropbox/PersonalComputer/LG%20%EA%B5%90%EC%9C%A1/LG_Day2_Reinforcement%20Learning2_%EA%B3%BC%EC%A0%9C.ipynb#Y121sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros((num_workers, num_tasks))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/test/Dropbox/PersonalComputer/LG%20%EA%B5%90%EC%9C%A1/LG_Day2_Reinforcement%20Learning2_%EA%B3%BC%EC%A0%9C.ipynb#Y121sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/test/Dropbox/PersonalComputer/LG%20%EA%B5%90%EC%9C%A1/LG_Day2_Reinforcement%20Learning2_%EA%B3%BC%EC%A0%9C.ipynb#Y121sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "E = []\n",
    "rewardList = []\n",
    "costList = []\n",
    "\n",
    "num_episode = 50000\n",
    "for e in range(num_episode):\n",
    "    \n",
    "    state = []\n",
    "    x = np.zeros((num_workers, num_tasks))\n",
    "        \n",
    "    total_reward = 0 \n",
    "    steps = 0\n",
    "    total_cost = 0\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        Action = Allocation.Get_Action(state)\n",
    "        x[Action, steps] += 1\n",
    "        \n",
    "        reward = Allocation.Get_State(x, costs, steps, Action, team1, team2)\n",
    "        next_state = copy.deepcopy(state)\n",
    "        next_state.append(Action)\n",
    "        Allocation.Q_learning(state, next_state, reward)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = copy.deepcopy(next_state)\n",
    "        total_cost += costs[Action][steps]\n",
    "        steps += 1\n",
    "\n",
    "        \n",
    "        if steps == num_tasks:\n",
    "            costList.append(total_cost)\n",
    "            E.append(e)\n",
    "            rewardList.append(total_reward)\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "pylab.plot(E, rewardList,'b')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
